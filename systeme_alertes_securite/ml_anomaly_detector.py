#!/usr/bin/env python3
"""
Module de D√©tection d'Anomalies par Machine Learning
Am√©lioration prioritaire pour le Syst√®me d'Alertes S√©curit√©

Fonctionnalit√©s:
- Apprentissage non supervis√© pour d√©tecter des patterns normaux
- D√©tection automatique d'anomalies comportementales  
- R√©duction drastique des faux positifs
- Analyse pr√©dictive des tendances
- Clustering des types d'alertes similaires
"""

import os
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import sqlite3
import pickle
import logging
from dataclasses import dataclass
from pathlib import Path

# Machine Learning imports
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-v0_8')

from colorama import init, Fore, Style
init(autoreset=True)

@dataclass
class AnomalyResult:
    """R√©sultat d'analyse d'anomalie"""
    is_anomaly: bool
    confidence_score: float
    anomaly_type: str
    cluster_id: int
    explanation: str
    features_importance: Dict[str, float]
    timestamp: datetime

@dataclass
class MLModelMetrics:
    """M√©triques d'un mod√®le ML"""
    model_type: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    false_positive_rate: float
    training_time: float
    last_retrain: datetime
    samples_count: int

class FeatureExtractor:
    """Extracteur de caract√©ristiques pour les alertes"""
    
    def __init__(self):
        self.label_encoders = {}
        self.tfidf_vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
        self.fitted = False
    
    def extract_temporal_features(self, timestamp: datetime) -> Dict[str, float]:
        """Extraire les caract√©ristiques temporelles"""
        return {
            'hour': timestamp.hour,
            'day_of_week': timestamp.weekday(),
            'day_of_month': timestamp.day,
            'month': timestamp.month,
            'is_weekend': 1 if timestamp.weekday() >= 5 else 0,
            'is_night': 1 if timestamp.hour < 6 or timestamp.hour > 22 else 0,
            'is_business_hours': 1 if 9 <= timestamp.hour <= 17 else 0
        }
    
    def extract_alert_features(self, alert_data: Dict[str, Any]) -> Dict[str, float]:
        """Extraire les caract√©ristiques d'une alerte"""
        features = {}
        
        # Caract√©ristiques temporelles
        timestamp = datetime.fromisoformat(alert_data['timestamp'])
        features.update(self.extract_temporal_features(timestamp))
        
        # Niveau d'alerte (encod√©)
        niveau_mapping = {'INFO': 0, 'WARNING': 1, 'ERROR': 2, 'CRITICAL': 3}
        features['niveau_encoded'] = niveau_mapping.get(alert_data.get('niveau', 'INFO'), 0)
        
        # Longueur du message
        message = alert_data.get('message', '')
        features['message_length'] = len(message)
        features['message_word_count'] = len(message.split())
        
        # Source (encod√©e)
        source = alert_data.get('source', 'unknown')
        if not self.fitted:
            # Mode training
            features['source_encoded'] = hash(source) % 1000
        else:
            # Mode inference
            features['source_encoded'] = self.label_encoders.get('source', {}).get(source, 0)
        
        # Caract√©ristiques des d√©tails
        details = alert_data.get('details', {})
        features['details_count'] = len(details)
        features['has_ip'] = 1 if any('ip' in str(k).lower() or 'ip' in str(v).lower() 
                                     for k, v in details.items()) else 0
        features['has_user'] = 1 if any('user' in str(k).lower() or 'user' in str(v).lower() 
                                       for k, v in details.items()) else 0
        features['has_file'] = 1 if any('file' in str(k).lower() or 'path' in str(k).lower() 
                                       for k, v in details.items()) else 0
        
        return features
    
    def extract_sequence_features(self, alerts: List[Dict[str, Any]]) -> Dict[str, float]:
        """Extraire les caract√©ristiques de s√©quence d'alertes"""
        if len(alerts) < 2:
            return {
                'sequence_length': len(alerts),
                'avg_time_between': 0,
                'niveau_variance': 0,
                'source_diversity': 1 if alerts else 0
            }
        
        # Trier par timestamp
        alerts_sorted = sorted(alerts, key=lambda x: datetime.fromisoformat(x['timestamp']))
        
        # Calculer les intervals de temps
        intervals = []
        for i in range(1, len(alerts_sorted)):
            t1 = datetime.fromisoformat(alerts_sorted[i-1]['timestamp'])
            t2 = datetime.fromisoformat(alerts_sorted[i]['timestamp'])
            intervals.append((t2 - t1).total_seconds())
        
        # Variance des niveaux
        niveau_mapping = {'INFO': 0, 'WARNING': 1, 'ERROR': 2, 'CRITICAL': 3}
        niveaux = [niveau_mapping.get(a.get('niveau', 'INFO'), 0) for a in alerts_sorted]
        
        # Diversit√© des sources
        sources = set(a.get('source', 'unknown') for a in alerts_sorted)
        
        return {
            'sequence_length': len(alerts),
            'avg_time_between': np.mean(intervals) if intervals else 0,
            'max_time_between': max(intervals) if intervals else 0,
            'min_time_between': min(intervals) if intervals else 0,
            'time_variance': np.var(intervals) if intervals else 0,
            'niveau_variance': np.var(niveaux),
            'niveau_trend': niveaux[-1] - niveaux[0] if len(niveaux) > 1 else 0,
            'source_diversity': len(sources) / len(alerts),
            'repetition_rate': 1 - (len(sources) / len(alerts))  # Plus de r√©p√©tition = plus suspect
        }
    
    def fit_transform(self, alerts_data: List[Dict[str, Any]]) -> np.ndarray:
        """Ajuster l'extracteur et transformer les donn√©es"""
        features_list = []
        
        # Collecter les caract√©ristiques individuelles
        for alert in alerts_data:
            features = self.extract_alert_features(alert)
            features_list.append(features)
        
        # Cr√©er un DataFrame pour faciliter le traitement
        df = pd.DataFrame(features_list)
        
        # Ajuster les encodeurs
        if not self.fitted:
            # Encoder les sources
            sources = [a.get('source', 'unknown') for a in alerts_data]
            unique_sources = list(set(sources))
            self.label_encoders['source'] = {source: idx for idx, source in enumerate(unique_sources)}
            
            # Re-encoder avec les vrais indices
            for i, alert in enumerate(alerts_data):
                source = alert.get('source', 'unknown')
                features_list[i]['source_encoded'] = self.label_encoders['source'].get(source, 0)
            
            df = pd.DataFrame(features_list)
            self.fitted = True
        
        return df.values
    
    def transform(self, alerts_data: List[Dict[str, Any]]) -> np.ndarray:
        """Transformer les donn√©es (mode inference)"""
        if not self.fitted:
            raise ValueError("L'extracteur doit √™tre ajust√© avant la transformation")
        
        features_list = []
        for alert in alerts_data:
            features = self.extract_alert_features(alert)
            features_list.append(features)
        
        df = pd.DataFrame(features_list)
        return df.values

class MLAnomalyDetector:
    """D√©tecteur d'anomalies par Machine Learning"""
    
    def __init__(self, db_path: str, model_dir: str = "./ml_models"):
        self.db_path = db_path
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        # Mod√®les ML
        self.isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        self.dbscan = DBSCAN(eps=0.5, min_samples=5)
        self.scaler = StandardScaler()
        self.feature_extractor = FeatureExtractor()
        
        # M√©triques et √©tat
        self.models_trained = False
        self.training_data_size = 0
        self.last_training = None
        self.metrics = {}
        
        # Configuration
        self.min_training_samples = 100
        self.retrain_threshold = 1000  # Nouveaux √©chantillons avant re-entra√Ænement
        self.anomaly_threshold = 0.1
        
        self.logger = self._setup_logging()
    
    def _setup_logging(self) -> logging.Logger:
        """Configuration du logging"""
        logger = logging.getLogger('MLAnomalyDetector')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.FileHandler(self.model_dir / 'ml_anomaly.log')
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def load_training_data(self, days_back: int = 30) -> List[Dict[str, Any]]:
        """Charger les donn√©es d'entra√Ænement depuis la base"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        cursor.execute('''
            SELECT id, timestamp, niveau, source, message, details, resolu
            FROM alertes 
            WHERE timestamp >= ?
            ORDER BY timestamp ASC
        ''', (cutoff_date,))
        
        rows = cursor.fetchall()
        conn.close()
        
        alerts_data = []
        for row in rows:
            alert = {
                'id': row[0],
                'timestamp': row[1],
                'niveau': row[2],
                'source': row[3],
                'message': row[4],
                'details': json.loads(row[5]) if row[5] else {},
                'resolu': bool(row[6])
            }
            alerts_data.append(alert)
        
        self.logger.info(f"Charg√© {len(alerts_data)} alertes pour l'entra√Ænement")
        return alerts_data
    
    def create_labels(self, alerts_data: List[Dict[str, Any]]) -> np.ndarray:
        """Cr√©er des labels pour l'entra√Ænement supervis√© (optionnel)"""
        # Utiliser des heuristiques pour √©tiqueter les anomalies
        labels = []
        
        for alert in alerts_data:
            is_anomaly = False
            
            # R√®gles heuristiques pour identifier les anomalies
            timestamp = datetime.fromisoformat(alert['timestamp'])
            
            # Alertes en dehors des heures normales
            if timestamp.hour < 6 or timestamp.hour > 22:
                is_anomaly = True
            
            # Alertes critiques r√©p√©titives
            if alert.get('niveau') == 'CRITICAL':
                is_anomaly = True
            
            # Messages tr√®s courts ou tr√®s longs (suspects)
            message_len = len(alert.get('message', ''))
            if message_len < 10 or message_len > 500:
                is_anomaly = True
            
            # Sources inhabituelles (bas√© sur fr√©quence)
            # TODO: Am√©liorer avec l'analyse de fr√©quence
            
            labels.append(1 if is_anomaly else 0)
        
        return np.array(labels)
    
    def train_models(self, alerts_data: Optional[List[Dict[str, Any]]] = None) -> Dict[str, MLModelMetrics]:
        """Entra√Æner tous les mod√®les ML"""
        if alerts_data is None:
            alerts_data = self.load_training_data()
        
        if len(alerts_data) < self.min_training_samples:
            raise ValueError(f"Pas assez de donn√©es pour l'entra√Ænement (minimum {self.min_training_samples})")
        
        start_time = datetime.now()
        self.logger.info(f"D√©but de l'entra√Ænement avec {len(alerts_data)} √©chantillons")
        
        # Extraire les caract√©ristiques
        X = self.feature_extractor.fit_transform(alerts_data)
        
        # Normaliser
        X_scaled = self.scaler.fit_transform(X)
        
        metrics = {}
        
        # 1. Entra√Æner Isolation Forest (d√©tection d'anomalies non supervis√©e)
        print(f"{Fore.CYAN}ü§ñ Entra√Ænement Isolation Forest...")
        if_start = datetime.now()
        self.isolation_forest.fit(X_scaled)
        if_time = (datetime.now() - if_start).total_seconds()
        
        # √âvaluer Isolation Forest
        anomaly_scores = self.isolation_forest.decision_function(X_scaled)
        predictions = self.isolation_forest.predict(X_scaled)
        anomaly_rate = np.sum(predictions == -1) / len(predictions)
        
        metrics['isolation_forest'] = MLModelMetrics(
            model_type="Isolation Forest",
            accuracy=0.0,  # Non applicable pour non supervis√©
            precision=0.0,
            recall=0.0,
            f1_score=0.0,
            false_positive_rate=anomaly_rate,
            training_time=if_time,
            last_retrain=datetime.now(),
            samples_count=len(alerts_data)
        )
        
        # 2. Entra√Æner DBSCAN (clustering)
        print(f"{Fore.CYAN}ü§ñ Entra√Ænement DBSCAN...")
        dbscan_start = datetime.now()
        cluster_labels = self.dbscan.fit_predict(X_scaled)
        dbscan_time = (datetime.now() - dbscan_start).total_seconds()
        
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        noise_ratio = np.sum(cluster_labels == -1) / len(cluster_labels)
        
        metrics['dbscan'] = MLModelMetrics(
            model_type="DBSCAN",
            accuracy=0.0,
            precision=0.0,
            recall=0.0,
            f1_score=0.0,
            false_positive_rate=noise_ratio,
            training_time=dbscan_time,
            last_retrain=datetime.now(),
            samples_count=len(alerts_data)
        )
        
        # Mettre √† jour l'√©tat
        self.models_trained = True
        self.training_data_size = len(alerts_data)
        self.last_training = datetime.now()
        self.metrics = metrics
        
        # Sauvegarder les mod√®les (apr√®s mise √† jour de l'√©tat)
        self.save_models()
        
        total_time = (datetime.now() - start_time).total_seconds()
        self.logger.info(f"Entra√Ænement termin√© en {total_time:.2f}s")
        
        # Afficher les r√©sultats
        self.print_training_summary(metrics, n_clusters, anomaly_rate, noise_ratio)
        
        return metrics
    
    def print_training_summary(self, metrics: Dict[str, MLModelMetrics], 
                             n_clusters: int, anomaly_rate: float, noise_ratio: float):
        """Afficher un r√©sum√© de l'entra√Ænement"""
        print(f"\n{Fore.GREEN}üéØ R√âSUM√â DE L'ENTRA√éNEMENT ML")
        print("=" * 50)
        
        print(f"{Fore.CYAN}üìä Isolation Forest:")
        print(f"   ‚Ä¢ Taux d'anomalies d√©tect√©es: {anomaly_rate:.1%}")
        print(f"   ‚Ä¢ Temps d'entra√Ænement: {metrics['isolation_forest'].training_time:.2f}s")
        
        print(f"\n{Fore.CYAN}üîç DBSCAN Clustering:")
        print(f"   ‚Ä¢ Nombre de clusters: {n_clusters}")
        print(f"   ‚Ä¢ Taux de bruit: {noise_ratio:.1%}")
        print(f"   ‚Ä¢ Temps d'entra√Ænement: {metrics['dbscan'].training_time:.2f}s")
        
        print(f"\n{Fore.GREEN}‚úÖ Mod√®les sauvegard√©s dans: {self.model_dir}")
    
    def detect_anomaly(self, alert_data: Dict[str, Any]) -> AnomalyResult:
        """D√©tecter si une alerte est une anomalie"""
        if not self.models_trained:
            return AnomalyResult(
                is_anomaly=False,
                confidence_score=0.5,
                anomaly_type="model_not_trained",
                cluster_id=-1,
                explanation="Les mod√®les ML ne sont pas encore entra√Æn√©s",
                features_importance={},
                timestamp=datetime.now()
            )
        
        # Extraire les caract√©ristiques
        X = self.feature_extractor.transform([alert_data])
        X_scaled = self.scaler.transform(X)
        
        # Pr√©diction Isolation Forest
        if_score = self.isolation_forest.decision_function(X_scaled)[0]
        if_prediction = self.isolation_forest.predict(X_scaled)[0]
        
        # Pr√©diction DBSCAN (cluster)
        cluster_id = self.dbscan.fit_predict(X_scaled)[0]
        
        # D√©terminer si c'est une anomalie
        is_anomaly = if_prediction == -1
        confidence_score = abs(if_score)  # Score de confiance
        
        # Type d'anomalie
        anomaly_type = "normal"
        if is_anomaly:
            if cluster_id == -1:
                anomaly_type = "outlier"
            else:
                anomaly_type = f"cluster_{cluster_id}"
        
        # Explication
        explanation = self.generate_explanation(alert_data, if_score, cluster_id)
        
        # Importance des caract√©ristiques (approximation)
        features_importance = self.calculate_feature_importance(X_scaled[0])
        
        result = AnomalyResult(
            is_anomaly=is_anomaly,
            confidence_score=confidence_score,
            anomaly_type=anomaly_type,
            cluster_id=cluster_id,
            explanation=explanation,
            features_importance=features_importance,
            timestamp=datetime.now()
        )
        
        self.logger.info(f"Anomalie d√©tect√©e: {is_anomaly}, Score: {confidence_score:.3f}, Type: {anomaly_type}")
        return result
    
    def generate_explanation(self, alert_data: Dict[str, Any], if_score: float, cluster_id: int) -> str:
        """G√©n√©rer une explication pour le r√©sultat"""
        explanations = []
        
        # Analyse temporelle
        timestamp = datetime.fromisoformat(alert_data['timestamp'])
        if timestamp.hour < 6 or timestamp.hour > 22:
            explanations.append("Alerte en dehors des heures normales")
        
        if timestamp.weekday() >= 5:
            explanations.append("Alerte en weekend")
        
        # Analyse du niveau
        niveau = alert_data.get('niveau', 'INFO')
        if niveau in ['CRITICAL', 'ERROR']:
            explanations.append(f"Niveau {niveau} inhabituel")
        
        # Score d'isolation
        if if_score < -0.5:
            explanations.append("Pattern tr√®s inhabituel d√©tect√©")
        elif if_score < 0:
            explanations.append("Pattern l√©g√®rement inhabituel")
        
        # Cluster
        if cluster_id == -1:
            explanations.append("Ne correspond √† aucun pattern connu")
        else:
            explanations.append(f"Similaire au groupe de patterns #{cluster_id}")
        
        return " ‚Ä¢ ".join(explanations) if explanations else "Pattern normal"
    
    def calculate_feature_importance(self, features: np.ndarray) -> Dict[str, float]:
        """Calculer l'importance approximative des caract√©ristiques"""
        feature_names = [
            'hour', 'day_of_week', 'day_of_month', 'month', 'is_weekend', 
            'is_night', 'is_business_hours', 'niveau_encoded', 'message_length',
            'message_word_count', 'source_encoded', 'details_count', 
            'has_ip', 'has_user', 'has_file'
        ]
        
        # Approximation simple bas√©e sur la d√©viation par rapport √† la moyenne
        importance = {}
        for i, name in enumerate(feature_names[:len(features)]):
            # Plus la valeur est √©loign√©e de 0 (apr√®s normalisation), plus elle est importante
            importance[name] = abs(float(features[i]))
        
        return importance
    
    def analyze_batch(self, alerts_data: List[Dict[str, Any]]) -> List[AnomalyResult]:
        """Analyser un lot d'alertes"""
        results = []
        
        print(f"{Fore.CYAN}üîç Analyse ML de {len(alerts_data)} alertes...")
        
        for alert in alerts_data:
            result = self.detect_anomaly(alert)
            results.append(result)
        
        # Statistiques
        anomalies = [r for r in results if r.is_anomaly]
        print(f"{Fore.GREEN}üìä R√©sultats: {len(anomalies)} anomalies d√©tect√©es sur {len(results)} alertes")
        
        return results
    
    def save_models(self):
        """Sauvegarder les mod√®les entra√Æn√©s"""
        models_to_save = {
            'isolation_forest': self.isolation_forest,
            'dbscan': self.dbscan,
            'scaler': self.scaler,
            'feature_extractor': self.feature_extractor
        }
        
        for name, model in models_to_save.items():
            filepath = self.model_dir / f"{name}.pkl"
            with open(filepath, 'wb') as f:
                pickle.dump(model, f)
        
        # Sauvegarder les m√©tadonn√©es
        metadata = {
            'training_data_size': self.training_data_size,
            'last_training': self.last_training.isoformat() if self.last_training else None,
            'models_trained': self.models_trained,  # Cette variable doit √™tre True apr√®s l'entra√Ænement
            'metrics': {k: {
                'model_type': v.model_type,
                'training_time': v.training_time,
                'samples_count': v.samples_count,
                'last_retrain': v.last_retrain.isoformat()
            } for k, v in self.metrics.items()}
        }
        
        with open(self.model_dir / 'metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        self.logger.info(f"Mod√®les sauvegard√©s dans {self.model_dir}")
    
    def load_models(self) -> bool:
        """Charger les mod√®les sauvegard√©s"""
        try:
            models_to_load = ['isolation_forest', 'dbscan', 'scaler', 'feature_extractor']
            
            for name in models_to_load:
                filepath = self.model_dir / f"{name}.pkl"
                if filepath.exists():
                    with open(filepath, 'rb') as f:
                        setattr(self, name, pickle.load(f))
                else:
                    return False
            
            # Charger les m√©tadonn√©es
            metadata_path = self.model_dir / 'metadata.json'
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                self.training_data_size = metadata.get('training_data_size', 0)
                self.last_training = datetime.fromisoformat(metadata['last_training']) if metadata.get('last_training') else None
                self.models_trained = metadata.get('models_trained', False)
                
                # Reconstruire les m√©triques
                self.metrics = {}
                for k, v in metadata.get('metrics', {}).items():
                    self.metrics[k] = MLModelMetrics(
                        model_type=v['model_type'],
                        accuracy=0.0,
                        precision=0.0,
                        recall=0.0,
                        f1_score=0.0,
                        false_positive_rate=0.0,
                        training_time=v['training_time'],
                        last_retrain=datetime.fromisoformat(v['last_retrain']),
                        samples_count=v['samples_count']
                    )
            
            self.logger.info("Mod√®les charg√©s avec succ√®s")
            return True
            
        except Exception as e:
            self.logger.error(f"Erreur lors du chargement des mod√®les: {e}")
            return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """Obtenir le statut des mod√®les ML"""
        return {
            'models_trained': self.models_trained,
            'training_data_size': self.training_data_size,
            'last_training': self.last_training.isoformat() if self.last_training else None,
            'models_dir': str(self.model_dir),
            'metrics': self.metrics
        }
    
    def should_retrain(self) -> bool:
        """D√©terminer s'il faut re-entra√Æner les mod√®les"""
        if not self.models_trained:
            return True
        
        # V√©rifier si assez de nouvelles donn√©es
        current_data_size = len(self.load_training_data(days_back=7))
        
        if current_data_size >= self.retrain_threshold:
            return True
        
        # V√©rifier l'anciennet√© du dernier entra√Ænement
        if self.last_training:
            days_since_training = (datetime.now() - self.last_training).days
            if days_since_training > 7:  # Re-entra√Æner chaque semaine
                return True
        
        return False
    
    def generate_analytics_report(self) -> Dict[str, Any]:
        """G√©n√©rer un rapport d'analyse ML"""
        if not self.models_trained:
            return {'error': 'Mod√®les non entra√Æn√©s'}
        
        # Analyser les donn√©es r√©centes
        recent_data = self.load_training_data(days_back=7)
        results = self.analyze_batch(recent_data)
        
        # Statistiques
        total_alerts = len(results)
        anomalies = [r for r in results if r.is_anomaly]
        anomaly_rate = len(anomalies) / total_alerts if total_alerts > 0 else 0
        
        # Analyse des types d'anomalies
        anomaly_types = {}
        for result in anomalies:
            anomaly_types[result.anomaly_type] = anomaly_types.get(result.anomaly_type, 0) + 1
        
        # Scores de confiance
        confidence_scores = [r.confidence_score for r in results]
        
        return {
            'period': '7 derniers jours',
            'total_alerts_analyzed': total_alerts,
            'anomalies_detected': len(anomalies),
            'anomaly_rate': anomaly_rate,
            'anomaly_types': anomaly_types,
            'avg_confidence': np.mean(confidence_scores),
            'confidence_std': np.std(confidence_scores),
            'model_status': self.get_model_status(),
            'recommendations': self._generate_recommendations(anomaly_rate, anomaly_types)
        }
    
    def _generate_recommendations(self, anomaly_rate: float, anomaly_types: Dict[str, int]) -> List[str]:
        """G√©n√©rer des recommandations bas√©es sur l'analyse"""
        recommendations = []
        
        if anomaly_rate > 0.3:
            recommendations.append("‚ö†Ô∏è Taux d'anomalies tr√®s √©lev√© - V√©rifier les r√®gles d'alerte")
        elif anomaly_rate > 0.15:
            recommendations.append("üìä Taux d'anomalies mod√©r√© - Surveillance recommand√©e")
        else:
            recommendations.append("‚úÖ Taux d'anomalies normal")
        
        if 'outlier' in anomaly_types and anomaly_types['outlier'] > 5:
            recommendations.append("üîç Beaucoup de patterns inconnus - Enrichir les donn√©es d'entra√Ænement")
        
        if self.should_retrain():
            recommendations.append("üîÑ Re-entra√Ænement des mod√®les recommand√©")
        
        return recommendations


def main():
    """Fonction principale pour tester le d√©tecteur ML"""
    import argparse
    
    parser = argparse.ArgumentParser(description="D√©tecteur d'Anomalies ML pour Alertes S√©curit√©")
    parser.add_argument("--db", default="../systeme_alertes_securite/alertes.db", help="Base de donn√©es des alertes")
    parser.add_argument("--train", action="store_true", help="Entra√Æner les mod√®les")
    parser.add_argument("--analyze", action="store_true", help="Analyser les alertes r√©centes")
    parser.add_argument("--report", action="store_true", help="G√©n√©rer un rapport d'analyse")
    parser.add_argument("--days", type=int, default=30, help="Nombre de jours pour les donn√©es")
    
    args = parser.parse_args()
    
    print(f"{Fore.BLUE}ü§ñ D√âTECTEUR D'ANOMALIES ML - ALERTES S√âCURIT√â")
    print("=" * 55)
    
    # Initialiser le d√©tecteur
    detector = MLAnomalyDetector(db_path=args.db)
    
    # Tenter de charger les mod√®les existants
    if detector.load_models():
        print(f"{Fore.GREEN}‚úÖ Mod√®les existants charg√©s")
    else:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Aucun mod√®le existant trouv√©")
    
    if args.train or not detector.models_trained:
        print(f"{Fore.CYAN}üöÄ Entra√Ænement des mod√®les ML...")
        try:
            metrics = detector.train_models()
            print(f"{Fore.GREEN}‚úÖ Entra√Ænement termin√© avec succ√®s")
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur lors de l'entra√Ænement: {e}")
            return
    
    if args.analyze:
        print(f"\n{Fore.CYAN}üîç Analyse des alertes r√©centes...")
        recent_alerts = detector.load_training_data(days_back=7)
        results = detector.analyze_batch(recent_alerts)
        
        # Afficher les anomalies d√©tect√©es
        anomalies = [r for r in results if r.is_anomaly]
        if anomalies:
            print(f"\n{Fore.RED}üö® ANOMALIES D√âTECT√âES:")
            for i, anomaly in enumerate(anomalies[:5], 1):
                print(f"{i}. Type: {anomaly.anomaly_type}")
                print(f"   Confiance: {anomaly.confidence_score:.3f}")
                print(f"   Explication: {anomaly.explanation}")
                print()
    
    if args.report:
        print(f"\n{Fore.CYAN}üìä G√©n√©ration du rapport d'analyse...")
        report = detector.generate_analytics_report()
        
        if 'error' not in report:
            print(f"\n{Fore.GREEN}üìà RAPPORT D'ANALYSE ML")
            print("-" * 30)
            print(f"P√©riode analys√©e: {report['period']}")
            print(f"Alertes analys√©es: {report['total_alerts_analyzed']}")
            print(f"Anomalies d√©tect√©es: {report['anomalies_detected']}")
            print(f"Taux d'anomalies: {report['anomaly_rate']:.1%}")
            print(f"Confiance moyenne: {report['avg_confidence']:.3f}")
            
            print(f"\n{Fore.CYAN}üîç Types d'anomalies:")
            for anomaly_type, count in report['anomaly_types'].items():
                print(f"  ‚Ä¢ {anomaly_type}: {count}")
            
            print(f"\n{Fore.YELLOW}üí° Recommandations:")
            for rec in report['recommendations']:
                print(f"  {rec}")
        else:
            print(f"{Fore.RED}‚ùå {report['error']}")


if __name__ == "__main__":
    main()